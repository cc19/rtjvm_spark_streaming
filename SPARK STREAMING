++++++++++SPARK STREAMING+++++++++++

SPARK Architecture:

applications        -> Streaming  | ML |   GraphX  | Other libraries
------------------------------------------------------------------------
high level
(structured APIs)   -> DataFrames | DataSets  |   Spark SQL
------------------------------------------------------------------------
low level APIs      -> Dstreams   |   RDDs    |   Distributed variables
------------------------------------------------------------------------

Spark - Gathers insights from a big data computation

> Why do we need Spark Streaming?
- Once we compute valuable, we need updates
- We need continuous big data processing => Spark Streaming

In batch processing, a fixed dataset is provided as input which is processed and it provides a fixed data as output.

Stream processing
- Includes new data to compute a result
- No definitive end of incoming data
- Batch Processing: operate on fixed dataset, compute result once
- In practice, stream and batch interoperate e.g.
:: incoming data joined with a fixed dataset
:: output of a streaming job periodically queried by a batch job
:: consistency between batch/streaming jobs (e.g bank account balance)

Real life examples/use cases:
- sensor readings
- interactions with an appliction/website
- credit card transactions
- real time dashboards
- alerts and notifications
- incremental big data
- incremental transactional data e.g. analytics or accounts
- interactive machine learning

Pros/Cons:

:: Pros:-
- Much lower latency than batch processing
- Greater performance/efficiency (especially with incremental data)

:: Cons:-
- Maintaining state and order of incoming data. Solution: Event time processing
- Exactly-once processing in the context of machine failures. Solution: Fault tolerance
- Responding to events at low latency
- Transactional data at runtime. Need to do it fast before the next stream of data arrives.
- Updating business logic at runtime. Need to do it fast before the next stream of data arrives.

Spark Streaming Principles:
Declarative API
- write "what" needs to be computed, let the library decide "how"
- alternative RaaT (record-at-a-time)
    > set of APIs to process each incoming element as it arrives
    > low level and high control API- maintaining state and resource usage is your responsibility
    > hard to develop

Event time vs Processing time API
- event time = when the event(record of data) was produced
- processing time = when the record arrives at the Spark engine
- event time is critical: allows detection of late data points with Watermarks
 
 Continuous vs Micro-batch processing
 - continuous = includes each data point as it arrives, we compute the data with every new record. Low latency because we only need to include one record with every recomputation.
 - micro-batch = wait for a few data points, process them all in the new result. Higher throughput i.e we can process multiple records at a time.

 Low Level DStreams vs High Level API (Structured streaming)

Spark streaming operates on micro batches.


# Spark Structured Streaming

Structured streaming I/O

High Level API
- ease of development
- interoperable with other SPark APIs
- auto optimizations

Spark Streaming Pricniples:
- Lazy evaluation
 Tranformations and Actions
    > Tranformations describe how DFs are obtained 
    > Actions start executing/running spark code
- Input sources
    > Kafka, Flume
    > Databases like PostGres, Cassandra
    > Distributed file system 
    > sockets
- Output sinks
    > Distributed file system
    > Databases
    > Kafka
    > Testing sinks e.g console, memory

Streaming I/O 
- Output modes
    > append = only add new records
    > update = modify records in place (if query has no aggregations, equivalent with append)
    > complete = rewrite everything
- Not all queries and sinks support all output modes
- Triggers = when new data is written
    > default: write as soon as the current micro-batch has been processed
    > once: write a single micro batch and stop
    > processing-time: look for new data at fixed intervals
    > continuous (currently experimental)

Streaming DataFrames: Summary
- Streaming DFs can be read via a spark session
- Streaming DFs have identical API to non streaming DF
- Streaming DFs can be written via a call to the start method

Aggregations: Summary
- Same aggregation API as non-streaming DFs
- Aggregations work at micro-batch level
- The "append" output mode not supported without Watermarks
- Some aggregations are not supported, e.g sorting, distinct, chained aggregations because for that spark will have to hold all data of that stream in memory

# Streaming Joins

Restrcited Joins
- stream joining with static: right outer join, full outer join, right semi join, right anti join are not permitted
- static joining with streaming: left outer join, full outer join, left semi join, left anti join are not permitted

For stream joins with stream
- inner joins supported
- left/right outer joins are supported, but must have Watermarks
- full outer joins are not supported

Streaming Joins: Summary
- Same join API as non streaming DFs
- Some join types are not supported (Refer 'Restrcited Joins' above)
- Stream-stream joins are supported (refer 'For stream joins with stream' above)

Streaming Datasets: Summary
- Same DS conversion as non streaming DS
- Streaming DSs support functional operators
- TradeOffs: 
    > pros: type safety, expressiveness
    > cons: potential perf implictions as lambdas cannot be optimized

Discretized Streams (DStreams): 
- Never ending sequence of RDDs 
    :: Nodes's clocks are synchronized
    :: batches are trigerred at the same time in the cluster
    :: each batch is an RDD
- Essentially a distributed collection of elements of the same type
    :: functional operators e.g map, flatMap, filter, reduce
    :: accessors to each RDD
    :: more advanced operators
- Needs a receiver to perform computations
    :: one receiver per DStream
    :: fetches data from the source , sends to spark, create blocks
    :: is managed by the Streaming Context on the driver
    :: occupies one core on the machine!

- The batch interval will decide when the spark engine will pull for new data if we want to read data into a DStream.
- The block interval will decide the interval in between which the partitions of the RDDs will be created.

DStreams: Summary
- DStreams are never ending sequence of RDDs
- Available under a streaming context where we can specify the batch interval
- We can create a DStream either from socket or text or from other sources
- Supports various transformations like map, flatMap, filter. etc.
- Computation started via an action+start of the streaming context

DS Tranformations: Summary
- Functional transformations like map, flatMap, filter
- For comprehensions
        for {
            line => dataSTream
            word => line.split(" ")
        } yield word.toUpperCase()
- Stream-oriented transformations like-
    count - Counting the number of records in each batch
    countByValue - DStream[K, V] counting the number of occurences of each value 
    reduceByKey - only on DStream[(K, V)]
    foreachRDD - allows us to process each rdd independently in whatever style we need


# Kafka

Apache Kafka is a distributed streaming platform. What exactly does that mean?

A streaming platform has three key capabilities:
- Publish and subscribe to streams of records, similar to a message queue or enterprise messaging system.
- Store streams of records in a fault-tolerant durable way.
- Process streams of records as they occur.

Kafka is generally used for two broad classes of applications:
- Building real-time streaming data pipelines that reliably get data between systems or applications
- Building real-time streaming applications that transform or react to the streams of data

- The Kafka cluster stores streams of records in categories called topics.
- Each record consists of a key, a value, and a timestamp.

Topics: 
A topic is a category or feed name to which records are published. 
Topics in Kafka are always multi-subscriber; that is, a topic can have zero, one, or many consumers that subscribe to the data written to it.

>> Enetring KAFKA console:
docker-compose up
docker exec -it 741 bash        //741.... is the container id of Kafka
bash-4.4# cd /opt/kafka_2.12-2.4.1

**Starting the producer
bash-4.4# bin/kafka-topics.sh --create --bootstrap-server localhost:9092 --replication-factor 1 --partitions 1 --topic chandua  //creating the producer
bash-4.4# bin/kafka-console-producer.sh --broker-list localhost:9092 --topic chandua
> <Enter your text here>

**Starting the consumer
docker exec -it 741 bash
bash-4.4# cd /opt/kafka_2.12-2.4.1
bash-4.4# bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic chandua

- For data to be kafka understandable, it needs to have a key and a value.

Kafka & Structured Streaming Summary:
- Read like any other data source. Need to specify bootstrap server and topic
- Write like any other data source. Need to specify bootstrap server, topic and checkpoint location

DStreams and Kafka Summary:
- More complex reading steps
- First we need to setup a map of Kafka parameters  and we also need to create a direct stream using Kafka utils. In its argument, we need to supply the spark streaming context, the prefer
consistent strategy and the subscribe consumer startegy. 
- More complex writing, per rdd, per partition
- We will need to process the stream before every partition and for every partition we need to set up a Kafka producer that is accessible by that executor which is processing the partition.
Then we need to go through the partition record by record and then we will need to send each record to Kafka.
- Points to remember during writing:
    > We need to create the Kafka producer inside the lambda that is processing each partition because otherwise the Kafka producers are not serializable, not transmissible throughout the
    spark cluster and writing will get screwed up.
    > We need to create a Kafka message for every record. 
    > When we are done, we need to close the producer so that the producer can flush its buffers.

# JDBC Integration
./psql.sh - For connecting to dockerized jdbc
Command inside the psql file: docker exec -it rockthejvm-sparkstreaming-postgres psql -U docker -d rtjvm

- We cannot read streams from JDBC
- We cannot write to JDBC in a streaming fashion
- But we can write batches using foreachBatch

# Cassandra Integration
./cql.sh - For connecting to dockerized cassandra single node cluster
Command inside the csql file: docker exec -it rockthejvm-sparkstreaming-cassandra cqlsh


# Event time windows

Event time: The moment when the record was generated
Processing time: The moment when the record arrives at the spark

Event time set by the data generation system. Usually a column in the dataset.

Window functions:
Aggregations on time based groups
Essential concepts -
* window durations
* window sliding intervals

As opposed to DStreams
* records are not necessarily taken between "now" and a certain past date
* we can control output modes

Summary:
- Can group streaming data into time-based groups
- Window duration and sliding interval must be a multiple of the batch interval 
- Output mode will influence results
- Tumbling window has no sliding window whreas Sliding window has.


# Processing Time windows: Summary

- Add prcoessing time to existing records
val linesDF = purchaseDF
            .select(col("value"), current_timestamp().as("Processing_time"))

- Do the window processing
* Spark doesnot care what the time based column mean
* The only criterion is that it should be of type timestamp

- Window duration and sliding interval must be a multiple of the batch interval
- The output mode chosen will influence results as usual


# Watermarks
= how far back we still consider records before dropping them

Basically, a watermark is a threshold to specify how long the system waits for late events. If an arriving event lies within the watermark, it gets used to update a query. Otherwise, if it’s older than the watermark, it will be dropped and not further processed by the Streaming Engine.

Assume watermark to be 10 mins
On receiving each batch of data, spark will calculate the maximum time and the watermark as 10 mins earlier.
Data Received at                            Max time                        Watermark (Max time - Watermark value)          Considered in computation
-----------------                          ----------                      ---------------------------------------         ---------------------------
 5.01                                       5.01                                    4.51                                                Y
 5.04                                       5.04                                    4.54                                                Y
 5.11                                       5.11                                    5.01                                                Y
 5.19                                       5.19                                    5.09                                                Y            
 5.08                                       5.19                                    5.09                                                N

Why do we need it?
- In distributed and networked systems, there’s always a chance for disruption — nodes going down, sensors are loosing connection and so on and so forth. Because of that, it’s not guaranteed that data will arrive in a Stream Processing Engine in the order they were created. For the sake of fault tolerance it’s therefore necessary to handle such Out-of-Order data.
To deal with this problem, the state of an aggregate must be preserved. In case a late event occurs, the query can then be reprocessed. But this means that the state of all aggregates must kept indefinitely, which causes the memory usage to grow to indefinitely too. And that is not practical in a real world scenario, unless the system has unlimited resources (resp. an unlimited budget). Therefore watermarking is a useful concept to constrain the system by design and prevent it from exploding at runtime.

Summary:
- Add watermarks to a time column 
val enhancedDF = purchaseDF.withWatermark("created", "2 seconds")

- With every batch, Spark will
* update the max time ever recorded
* update watermark as (max time - watermark duration)

- Gurantees
* in every batch, all records with time > watermark, will be considered
* if using window functions, a window will be updated untill the watermark surpasses the window

- No Gurantees
* records whose time < watermark, will not necessarily be dropped though they are highly likely to.

- Aggregations and joins in append mode need watermarks
* a watermark allows spark to drop old records from state management.

- withWatermark must be called on the same column as the timestamp column used in the aggregate.
For example, df.withWatermark("time", "1 min").groupBy("time2").count() is invalid in Append output mode, as watermark is defined on a different column from the aggregation column.

